<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>gpm scheduler on Go设计实现内幕</title>
    <link>https://hitzhangjie.pro/tags/gpm-scheduler/</link>
    <description>Recent content in gpm scheduler on Go设计实现内幕</description>
    <generator>Hugo -- gohugo.io</generator><atom:link href="https://hitzhangjie.pro/tags/gpm-scheduler/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>GOMAXPROCS &amp; live updates</title>
      <link>https://hitzhangjie.pro/posts/GOMAXPROCS-live-updates/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hitzhangjie.pro/posts/GOMAXPROCS-live-updates/</guid>
      <description>Let&amp;rsquo;s Summarize #  介绍了GOMAXPROCS的作用，它控制了最多同时有多少个操作系统线程并发执行。除了这个环境变量，也可以在运行时通过runtime.MAXPROCS(n)来设置，也就是说允许live update（实时更新）。
该值默认等于可见的CPU核数，但是在docker容器里面希望做些资源隔离，比如希望go程序将最大并发执行操作系统线程数设置为分配给容器的CPU核数，而非母机的CPU核数，实际开发过程中有遇到因为容器中获取的不是分配的实际核数导致的调度问题。
GOMAXPROCS通常建议设置为可见CPU数或者核数。如果设置值偏大，则更多的线程被创建，操作系统调度多线程到有限的处理器核上将会引入更多的上下文切换开销，切换后协程相关的调度问题也将引入更多的开销。如果设置更小，并发度低，显而易见不利于获得更高性能。
ps：uber开源了一个库来专门获取、设置容器内CPU的数量。 see https://github.com/uber-go/automaxprocs
Source Analysis #  References #   https://medium.com/a-journey-with-go/go-gomaxprocs-live-updates-407ad08624e1  </description>
    </item>
    
    <item>
      <title>scheduling in go: part i - os scheduler</title>
      <link>https://hitzhangjie.pro/posts/scheduling-in-go-part-i-os-scheduler/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hitzhangjie.pro/posts/scheduling-in-go-part-i-os-scheduler/</guid>
      <description>Let&amp;rsquo;s Summarize #  本文是这个文集中的第一篇，以操作系统调度器为例，介绍了调度过程中涉及的一些内容：
 PC：线程切换后如何恢复下一条待执行指令的地址； 线程状态：线程是running、runnable、waiting状态的，调度器可以调度哪些线程，哪些不能； 任务类型：计算密集型、IO密集型，通常后者才会牵扯到任务调度问题； 上下文切换：上下文切换也有协作式或者抢占式之分，上下文切换是有开销的，如线程切换一次大约为1.4微秒左右，这点时间现代处理器可以执行1.2w~1.8w条指令，实际值可能会更高； 多少个线程合适，过多的线程会给线程调度带来开销，需要根据处理器核数、线程数、处理任务之间寻找一个相对合理的平衡值； cache一致性问题，现代处理器存储层次中存在多级cache，多线程程序需要考虑cache一致性问题。cache本身会提高性能，大牛市如果线程切换到不同核执行，可能会带来cache失效问题，也会引入开销； 调度决策，如何选择下一个待调度的线程执行；  本文相当于是从操作系统线程的角度出发，将调度器设计实现过程中可能遇到的一些主要问题都做了提纲挈领的一个概述，这些问题虽然是围绕着线程来说的，但是这也是实现一个优秀的协程调度器需要考虑的。
这篇文章将这些，都是为后文深入介绍go运行时调度器做铺垫的。
Source Analysis #  References #   https://www.ardanlabs.com/blog/2018/08/scheduling-in-go-part1.html  </description>
    </item>
    
    <item>
      <title>scheduling in go: part ii - go scheduler</title>
      <link>https://hitzhangjie.pro/posts/scheduling-in-go-part-ii-go-scheduler/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hitzhangjie.pro/posts/scheduling-in-go-part-ii-go-scheduler/</guid>
      <description>Let&amp;rsquo;s Summarize #  GMP调度模型，P代表了逻辑核的数量，比如Intel酷睿i7处理器是支持超线程的，一个超线程可以认为是一个虚拟核，go运行时看到的核数实际是虚拟核的数量。/// GMP调度模型和操作系统调度器类比，后者是将线程调度到不同的处理器核上执行，前者是将不同的goroutines调度到不同的线程上执行。/// 操作系统调度器是抢占式调度，go运行时调度器在以前版本（1.14以前）是协作式调度，后面也支持了抢占式调度。/// go调度器什么时候检查要不要执行goroutines切换呢？有这么几个时机，使用go func(){}创建新的协程，执行GC，执行系统调用，执行同步操作。/// 介绍了上述几种情况下的一些调度细节。/// work stealing调度器工作方式，为什么没有用work sharing，后者需要用把全局锁或者其他同步措施来同步，是有开销的，而且还比较明显，另外容易破坏goroutines的调度粘性，影响执行效率。
Source Analysis #  References #   https://www.ardanlabs.com/blog/2018/08/scheduling-in-go-part2.html  </description>
    </item>
    
    <item>
      <title>scheduling in go: part iii - concurrency</title>
      <link>https://hitzhangjie.pro/posts/scheduling-in-go-part-iii-concurrency/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hitzhangjie.pro/posts/scheduling-in-go-part-iii-concurrency/</guid>
      <description>Let&amp;rsquo;s Summarize #  介绍了并行和并发的区别，介绍了工作负载类型的差别以及适合采用的提高效率的方式，计算密集型（CPU-bound）适合通过并行计算来提高效率，IO密集型（IO-bound）适合通过并发来提高计算效率。
Source Analysis #  References #   https://www.ardanlabs.com/blog/2018/12/scheduling-in-go-part3.html  </description>
    </item>
    
  </channel>
</rss>
