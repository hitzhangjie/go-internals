<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Go设计实现内幕</title><link>https://hitzhangjie.pro/go-internals/docs/runtime/Scheduler/</link><description>Recent content on Go设计实现内幕</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://hitzhangjie.pro/go-internals/docs/runtime/Scheduler/index.xml" rel="self" type="application/rss+xml"/><item><title>concurrency &amp; scheduler affinity</title><link>https://hitzhangjie.pro/go-internals/docs/runtime/Scheduler/concurrency-scheduler-affinity/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hitzhangjie.pro/go-internals/docs/runtime/Scheduler/concurrency-scheduler-affinity/</guid><description>Let&amp;rsquo;s Summarize # GM模型
go1.1之前的调度模型是GM模型，有一个全局goroutine队列global queue，它的问题是：
访问global queue中的g，需要通过一个全局锁，锁粒度大，锁竞争严重； global queue中的g被恢复执行后，不一定在原来的线程上恢复，也不一定在原来的核上执行，cache命中率低； GMP模型
go1.1开始，引入了新的调度器实现GMP模型，引入了一个局部的local queue，这个改进能够在P local queue有g时避免去锁定global queue，也就减少了锁定了整个scheduler的情况。
goroutine调度粘性
破坏了调度粘性
因为引入了P，M会优先执行P下的local queue中的g，g调度到哪个m执行就有了一定的调度粘性（affinity），但是这个调度粘性也会在某几种情况下被打破，比如：
GMP是一个work-stealing调度器，m会在本地P local queue空时，尝试从其他地方获取一部分g来运行，比如从global queue，这里没有的话还会从netpoller中获取网络IO事件就绪的g，再没有就从其他P的local queue中获取一部分，这样相当于其他P的g没有在原来P关联的M上执行，打破了这里的调度粘性；
系统调用，当一个系统调用发生时（文件操作、http调用、数据库操作等），go会将当前正在运行的操作系统线程给挂起（不可中断等待状态），并且会创建一个新的操作系统线程M来处理这个P上的local queue中的g，后续执行系统调用的线程恢复后有可能会去处理其他P上的g，这也打破了这里的调度粘性；
保护调度粘性
为了减少打破这里的调度粘性，这两个限制可以尽可能去避免，以优化性能。
g、m之间的粘性，m、p之间的粘性，说白了都跟运行时依赖的缓存数据有关系，如果打破了粘性，缓存命中率下降，性能就会受影响，这个很好理解。所以go中也针对上述问题做了优化，比如一个陷入阻塞系统调用的m恢复执行后可以通过steal或者retake的方式来争取获取原来的P，也是一个办法吧，这部分了解就可以了。
对于chan send/recv引起的goroutine阻塞恢复问题，如果goroutine恢复后排在P的local queue的末尾，如果前面有其他goroutine执行，那么大概率这个goroutine会被其他的P上的M偷走，那么当前这个g就要和之前的M、P脱离关系了，一些缓存数据就无法复用了，为了减少打破这里的粘性，赋予了chan阻塞恢复的g更大的调度优先级，比如不将其放到P的local queue，而是将其放到P的runnext中，这样下次就可以立即执行了。
调度粘性的优势
p上有mcache、gFree，m上有tls，m运行g申请小于32K的内存是从p.mcache中分配，维持g、m、p之间的关系有助于复用之前p上建立的mcache，也有助于m创建新的g时复用p上之前维护的空闲g列表。
当然可能还有一些其他的原因，这里暂时先不展开了，想全了再展开。TODO
see：https://sourcegraph.com/github.com/golang/go/-/blob/src/runtime/runtime2.go#L613
Source Analysis # References # https://medium.com/a-journey-with-go/go-concurrency-scheduler-affinity-3b678f490488</description></item><item><title>GOMAXPROCS &amp; live updates</title><link>https://hitzhangjie.pro/go-internals/docs/runtime/Scheduler/GOMAXPROCS-live-updates/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hitzhangjie.pro/go-internals/docs/runtime/Scheduler/GOMAXPROCS-live-updates/</guid><description>Let&amp;rsquo;s Summarize # 介绍了GOMAXPROCS的作用，它控制了最多同时有多少个操作系统线程并发执行。除了这个环境变量，也可以在运行时通过runtime.MAXPROCS(n)来设置，也就是说允许live update（实时更新）。
该值默认等于可见的CPU核数，但是在docker容器里面希望做些资源隔离，比如希望go程序将最大并发执行操作系统线程数设置为分配给容器的CPU核数，而非母机的CPU核数，实际开发过程中有遇到因为容器中获取的不是分配的实际核数导致的调度问题。
GOMAXPROCS通常建议设置为可见CPU数或者核数。如果设置值偏大，则更多的线程被创建，操作系统调度多线程到有限的处理器核上将会引入更多的上下文切换开销，切换后协程相关的调度问题也将引入更多的开销。如果设置更小，并发度低，显而易见不利于获得更高性能。
ps：uber开源了一个库来专门获取、设置容器内CPU的数量。 see https://github.com/uber-go/automaxprocs
Source Analysis # References # https://medium.com/a-journey-with-go/go-gomaxprocs-live-updates-407ad08624e1</description></item><item><title>scheduling in go: part i - os scheduler</title><link>https://hitzhangjie.pro/go-internals/docs/runtime/Scheduler/scheduling-in-go-part-i-os-scheduler/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hitzhangjie.pro/go-internals/docs/runtime/Scheduler/scheduling-in-go-part-i-os-scheduler/</guid><description>Let&amp;rsquo;s Summarize # 本文是这个文集中的第一篇，以操作系统调度器为例，介绍了调度过程中涉及的一些内容：
PC：线程切换后如何恢复下一条待执行指令的地址； 线程状态：线程是running、runnable、waiting状态的，调度器可以调度哪些线程，哪些不能； 任务类型：计算密集型、IO密集型，通常后者才会牵扯到任务调度问题； 上下文切换：上下文切换也有协作式或者抢占式之分，上下文切换是有开销的，如线程切换一次大约为1.4微秒左右，这点时间现代处理器可以执行1.2w~1.8w条指令，实际值可能会更高； 多少个线程合适，过多的线程会给线程调度带来开销，需要根据处理器核数、线程数、处理任务之间寻找一个相对合理的平衡值； cache一致性问题，现代处理器存储层次中存在多级cache，多线程程序需要考虑cache一致性问题。cache本身会提高性能，大牛市如果线程切换到不同核执行，可能会带来cache失效问题，也会引入开销； 调度决策，如何选择下一个待调度的线程执行； 本文相当于是从操作系统线程的角度出发，将调度器设计实现过程中可能遇到的一些主要问题都做了提纲挈领的一个概述，这些问题虽然是围绕着线程来说的，但是这也是实现一个优秀的协程调度器需要考虑的。
这篇文章将这些，都是为后文深入介绍go运行时调度器做铺垫的。
Source Analysis # References # https://www.ardanlabs.com/blog/2018/08/scheduling-in-go-part1.html</description></item><item><title>scheduling in go: part ii - go scheduler</title><link>https://hitzhangjie.pro/go-internals/docs/runtime/Scheduler/scheduling-in-go-part-ii-go-scheduler/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hitzhangjie.pro/go-internals/docs/runtime/Scheduler/scheduling-in-go-part-ii-go-scheduler/</guid><description>Let&amp;rsquo;s Summarize # GMP调度模型，P代表了逻辑核的数量，比如Intel酷睿i7处理器是支持超线程的，一个超线程可以认为是一个虚拟核，go运行时看到的核数实际是虚拟核的数量。/// GMP调度模型和操作系统调度器类比，后者是将线程调度到不同的处理器核上执行，前者是将不同的goroutines调度到不同的线程上执行。/// 操作系统调度器是抢占式调度，go运行时调度器在以前版本（1.14以前）是协作式调度，后面也支持了抢占式调度。/// go调度器什么时候检查要不要执行goroutines切换呢？有这么几个时机，使用go func(){}创建新的协程，执行GC，执行系统调用，执行同步操作。/// 介绍了上述几种情况下的一些调度细节。/// work stealing调度器工作方式，为什么没有用work sharing，后者需要用把全局锁或者其他同步措施来同步，是有开销的，而且还比较明显，另外容易破坏goroutines的调度粘性，影响执行效率。
Source Analysis # References # https://www.ardanlabs.com/blog/2018/08/scheduling-in-go-part2.html</description></item><item><title>scheduling in go: part iii - concurrency</title><link>https://hitzhangjie.pro/go-internals/docs/runtime/Scheduler/scheduling-in-go-part-iii-concurrency/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hitzhangjie.pro/go-internals/docs/runtime/Scheduler/scheduling-in-go-part-iii-concurrency/</guid><description>Let&amp;rsquo;s Summarize # 介绍了并行和并发的区别，介绍了工作负载类型的差别以及适合采用的提高效率的方式，计算密集型（CPU-bound）适合通过并行计算来提高效率，IO密集型（IO-bound）适合通过并发来提高计算效率。
Source Analysis # References # https://www.ardanlabs.com/blog/2018/12/scheduling-in-go-part3.html</description></item><item><title>work-stealing in go scheduler</title><link>https://hitzhangjie.pro/go-internals/docs/runtime/Scheduler/work-stealing-in-go-scheduler/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hitzhangjie.pro/go-internals/docs/runtime/Scheduler/work-stealing-in-go-scheduler/</guid><description>Let&amp;rsquo;s Summarize # GMP调度模型中，每个P的local queue长度为256，创建g时，如果p的local queue满了，就会往global queue里面放了。
为了保证各个P的负载均衡，采用work-stealing调度方式，当一个P上的M要执行一个G时，G从哪里获取呢？
尝试从P的local queue获取，没有则走下一步； 尝试从global queue获取，没有则走下一步； 尝试从netpoller获取，没有则走下一步； 尝试从其他P的local queue中获取，没有就真的没有了； 如果真的没有，就要考虑idleM、idleP的处理了。
ps：系统调用阻塞的goroutine恢复执行时，会将其加入到global queue中，因此其可能破坏了原来的GMP之间的调度粘性，可能在不同的P上由不同的M来执行。
Source Analysis # References # https://medium.com/a-journey-with-go/go-work-stealing-in-go-scheduler-d439231be64d</description></item></channel></rss>